{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import *\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex patterns and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: It's important to prefix your regex patterns with r to ensure that your patterns are interpreted in the way you want them to. Else, you may encounter problems to do with escape sequences in strings.*\n",
    "\n",
    "*For example, \"\\n\" in Python is used to indicate a new line, but if you use the r prefix, it will be interpreted as the raw string \"\\n\" - that is, the character \"\\\" followed by the character \"n\" - and not as a new line.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Let', 's', 'write', 'RegEx']\n"
     ]
    }
   ],
   "source": [
    "my_string = \"Let's write RegEx!\"\n",
    "\n",
    "# find words in a string\n",
    "pattern=r'\\w+'\n",
    "print(re.findall(pattern, my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n"
     ]
    }
   ],
   "source": [
    "my_string=\"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\"\n",
    "\n",
    "# Split a string on sentence endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "print(re.split(sentence_endings, my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "# Find all digits in a string\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n"
     ]
    }
   ],
   "source": [
    "# Find all capitalized words in a string\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n"
     ]
    }
   ],
   "source": [
    "# Split a string on spaces\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex + Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SOLDIER', '#1', 'Found', 'them', '?', 'In', 'Mercea', '?', 'The', 'coconut', 's', 'tropical', '!']\n"
     ]
    }
   ],
   "source": [
    "my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
    "\n",
    "# Tokenize string by words,#digits,?,!\n",
    "pattern = r\"(\\w+|#\\d|\\?|!)\"\n",
    "print(regexp_tokenize(my_string, pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#nlp', '#python']\n"
     ]
    }
   ],
   "source": [
    "tweet= 'This is the best #nlp exercise ive found online! #python'\n",
    "\n",
    "# find hashtags (#) in tweet\n",
    "pattern1 = r\"#\\w+\" \n",
    "print(regexp_tokenize(tweet,pattern1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@datacamp', '#nlp', '#python']\n"
     ]
    }
   ],
   "source": [
    "tweet='Thanks @datacamp :) #nlp #python'\n",
    "\n",
    "# find mentions (@) and hashtags (#) in tweet\n",
    "pattern2 = r\"(@\\w+|#\\w+)\"\n",
    "print(regexp_tokenize(tweet,pattern2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning']]\n"
     ]
    }
   ],
   "source": [
    "tweets=['This is the best #nlp exercise ive found online! #python',\n",
    "         '#NLP is super fun! <3 #learning']\n",
    "\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "\n",
    "tknzr = TweetTokenizer() # an instance of TweetTokenizer\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-ascii Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: Unicode ranges for emoji are:*\n",
    "\n",
    "*('\\U0001F300'-'\\U0001F5FF'), ('\\U0001F600-\\U0001F64F'), ('\\U0001F680-\\U0001F6FF'), and ('\\u2600'-\\u26FF-\\u2700-\\u27BF')*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', 'ðŸ•', 'Und', 'fÃ¤hrst', 'du', 'mit', 'Ãœber', '?', 'ðŸš•']\n",
      "['Wann', 'Pizza', 'Und', 'Ãœber']\n",
      "['ðŸ•', 'ðŸš•']\n"
     ]
    }
   ],
   "source": [
    "german_text = 'Wann gehen wir Pizza essen? ðŸ• Und fÃ¤hrst du mit Ãœber? ðŸš•'\n",
    "\n",
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-ZÃœ]\\w+\"\n",
    "print(regexp_tokenize(german_text,capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text,emoji))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charting word length with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SCENE 1: [wind] [clop clop clop] ', 'KING ARTHUR: Whoa there!  [clop clop clop] ', 'SOLDIER #1: Halt!  Who goes there?', 'ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!']\n"
     ]
    }
   ],
   "source": [
    "holy_grail=\"SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\"\n",
    "\n",
    "# Split the script into lines\n",
    "lines = holy_grail.split('\\n')\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SCENE 1: [wind] [clop clop clop] ', ' Whoa there!  [clop clop clop] ', ' Halt!  Who goes there?', ' It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!']\n"
     ]
    }
   ],
   "source": [
    "# Replace all script lines for speaker\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['SCENE', '1', 'wind', 'clop', 'clop', 'clop'], ['Whoa', 'there', 'clop', 'clop', 'clop'], ['Halt', 'Who', 'goes', 'there'], ['It', 'is', 'I', 'Arthur', 'son', 'of', 'Uther', 'Pendragon', 'from', 'the', 'castle', 'of', 'Camelot', 'King', 'of', 'the', 'Britons', 'defeator', 'of', 'the', 'Saxons', 'sovereign', 'of', 'all', 'England']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize each line\n",
    "tokenized_lines = [regexp_tokenize(s,'\\w+') for s in lines]\n",
    "print(tokenized_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a histogram of the line lengths\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "plt.hist(line_num_words)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[datacamp](https://github.com/AparGarg99/Tutorials/files/5988939/regex.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
