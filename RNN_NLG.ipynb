{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_z0DEGAEpWo"
   },
   "source": [
    "We will build a character-wise RNN trained on Anna Karenina dataset. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "<img src=\"http://karpathy.github.io/assets/rnn/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K613cO_SEpWp"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This tutorial uses tensorflow 1.x\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ic2RUo52EpWw"
   },
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BWYik6ZVEpWw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1985223"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll convert text into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'h', 'x', '-', '(', 'a', 'b', '\\n', 'P', '@', 'j', '5', 'i', 'z', '0', 'y', '4', '2', '.', 'w', 'v', 'C', 's', 'N', 'r', 'T', 'Y', ')', 'o', ' ', 'V', 'q', 'p', '6', '8', '\"', 'K', 'O', '`', 'g', 'Z', ';', 't', 'S', 'M', 'U', '!', '$', 'k', 'H', '&', 'I', ':', 'l', ',', '1', 'm', 'e', 'L', 'n', '/', 'X', 'J', 'D', '*', '7', 'f', '9', '%', 'G', \"'\", 'Q', 'c', 'W', 'A', 'E', 'B', 'u', 'd', '3', 'F', 'R', '?', '_'}\n"
     ]
    }
   ],
   "source": [
    "# create vocabulary of unique characters\n",
    "vocab = set(text)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'h', 1: 'x', 2: '-', 3: '(', 4: 'a', 5: 'b', 6: '\\n', 7: 'P', 8: '@', 9: 'j', 10: '5', 11: 'i', 12: 'z', 13: '0', 14: 'y', 15: '4', 16: '2', 17: '.', 18: 'w', 19: 'v', 20: 'C', 21: 's', 22: 'N', 23: 'r', 24: 'T', 25: 'Y', 26: ')', 27: 'o', 28: ' ', 29: 'V', 30: 'q', 31: 'p', 32: '6', 33: '8', 34: '\"', 35: 'K', 36: 'O', 37: '`', 38: 'g', 39: 'Z', 40: ';', 41: 't', 42: 'S', 43: 'M', 44: 'U', 45: '!', 46: '$', 47: 'k', 48: 'H', 49: '&', 50: 'I', 51: ':', 52: 'l', 53: ',', 54: '1', 55: 'm', 56: 'e', 57: 'L', 58: 'n', 59: '/', 60: 'X', 61: 'J', 62: 'D', 63: '*', 64: '7', 65: 'f', 66: '9', 67: '%', 68: 'G', 69: \"'\", 70: 'Q', 71: 'c', 72: 'W', 73: 'A', 74: 'E', 75: 'B', 76: 'u', 77: 'd', 78: '3', 79: 'F', 80: 'R', 81: '?', 82: '_'}\n"
     ]
    }
   ],
   "source": [
    "# indexing of characters in vocabulary\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "print(int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20  0  4 31 41 56 23 28 54  6  6  6 48  4 31 31 14 28 65  4 55 11 52 11\n",
      " 56 21 28  4 23 56 28  4 52 52 28  4 52 11 47 56 40 28 56 19 56 23 14 28\n",
      " 76 58  0  4 31 31 14 28 65  4 55 11 52 14 28 11 21 28 76 58  0  4 31 31\n",
      " 14 28 11 58 28 11 41 21 28 27 18 58  6 18  4 14 17  6  6 74 19 56 23 14\n",
      " 41  0 11 58]\n"
     ]
    }
   ],
   "source": [
    "# Convert text data to numeric format \n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)\n",
    "print(chars[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FG7Y_cJkEpXA"
   },
   "source": [
    "Since the network is working with individual characters, it's similar to a classification problem in which we are trying to predict the next character from the previous text.  Here's how many 'classes' our network has to pick from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_eKkopoLEpXE"
   },
   "source": [
    "# Make Training and Validation batches\n",
    "\n",
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the batch size. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6CidnLd9EpXF"
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # This is a Supervised learning problem\n",
    "    # For every character prediction is the next character\n",
    "    # If the word is HELLO, X = H and Y = E, X = E and Y = L and so on\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the first split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qm4BLeOBEpXQ",
    "outputId": "c16ddb7c-5251-47db-bad3-01407ca088dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178650)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "#-----------\n",
    "# Total characters = 1786500 (after dropping some characters from the text to make full batches)\n",
    "# 1786500 characters are divided into batches of 10 (batch_size). Therefore, each batch has 178650 characters.\n",
    "# from each batch of 178650 characters, send sub-batches of 50 (num_steps) into NN.\n",
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 50)\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OSjjjviQEpXg"
   },
   "source": [
    "I'll write another function to grab batches out of the arrays made by `split_data`. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FJDtR5-qEpXi"
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8mn8kXBREpXo"
   },
   "source": [
    "# Model Development\n",
    "\n",
    "Below is a function where I build the graph for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0mEKB2r8EpXq"
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "    '''\n",
    "    Arguments\n",
    "    ---------\n",
    "    num_classes = vocab size\n",
    "    batch_size - Number of characters/sequences running through the network in one pass.\n",
    "    num_steps - Number of characters in the sequence the network is trained on. \n",
    "                Larger is better typically, the network will learn more long range dependencies. \n",
    "                But it takes longer to train. 100 is typically a good number here.\n",
    "    lstm_size - The number of cells/units in each LSTM layer.\n",
    "    num_layers - Number of LSTM layers.\n",
    "    learning_rate - Learning rate for training\n",
    "    '''\n",
    "    \n",
    "    # When we're using this network for sampling later, we'll be passing in\n",
    "    # one character at a time, so providing an option for that\n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # One-hot encoding the input and target characters\n",
    "    x_one_hot = tf.one_hot(inputs, num_classes)\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "\n",
    "    ###################################### Build the RNN layers #############################################\n",
    "    \n",
    "    # Basic LSTM layer\n",
    "    def make_layer(lstm_size):\n",
    "        return tf.contrib.rnn.BasicLSTMCell(lstm_size, state_is_tuple = True)\n",
    "    \n",
    "    \n",
    "    # Stacked RNN\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([make_layer(lstm_size) for _ in range(num_layers)], state_is_tuple = True)\n",
    "    \n",
    "    \n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "\n",
    "    ### Run the data through the RNN layers\n",
    "    # This makes a list where each element is on step in the sequence\n",
    "    rnn_inputs = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, 1)]\n",
    "    \n",
    "    # Run each sequence step through the RNN and collect the outputs\n",
    "    outputs, state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one output row for each step for each batch\n",
    "    seq_output = tf.concat(outputs, axis=1)\n",
    "    output = tf.reshape(seq_output, [-1, lstm_size])\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and batch\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    # Reshape the targets to match the logits\n",
    "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # Export the nodes\n",
    "    # NOTE: I'm using a namedtuple here because I think they are cool\n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HfSgTtkJEpXx"
   },
   "source": [
    "# Tips and Tricks for Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. \n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network. I'm going to write it in here for your benefit, but also link to [where it originally came from](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `lstm_size` and `num_layers`. I would advise that you always use `num_layers` of either 2/3. The `lstm_size` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - **Number of parameters in model** - 1 million parameters is 1 million characters. This is printed when you start training.\n",
    "> - **Size of dataset** - 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - If dataset = 100MB (100 million characters), parameters = 150K (0.15 million characters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `lstm_size` larger.\n",
    "> - If dataset = 10MB (10 million characters), parameters = 10 million (10 million characters). In this case I'll have to monitor my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l9SkzwoZEpX-"
   },
   "source": [
    "# Training\n",
    "\n",
    "Time for training which is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint.\n",
    "\n",
    "Here I'm saving checkpoints with the format\n",
    "\n",
    "`i{iteration number}_l{# hidden layer units}_v{validation loss}.ckpt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QQyGessrEpX4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file checkpoints already exists.\n"
     ]
    }
   ],
   "source": [
    "# create a checkpointing directory\n",
    "!mkdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JblY0l8zEpXy"
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100 \n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "keep_prob = 10.5\n",
    "epochs = 2\n",
    "save_every_n = 200 # Save model every N iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "foB5jui2EpYA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-12-7228d6e3273e>:38: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-12-7228d6e3273e>:42: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From C:\\Users\\Amit\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py:180: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "WARNING:tensorflow:From <ipython-input-12-7228d6e3273e>:53: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\Amit\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Amit\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-12-7228d6e3273e>:74: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Amit\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "erLbyWvJEpYR",
    "outputId": "a804d5eb-0092-4d3b-d2d9-8c2a5cde9042",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################# Epoch 1 #############################\n",
      "Epoch 1/2  Iteration 1/356 Training loss: 4.4202 12.7677 sec/batch\n",
      "Epoch 1/2  Iteration 2/356 Training loss: 4.3765 2.1770 sec/batch\n",
      "Epoch 1/2  Iteration 3/356 Training loss: 4.2056 2.3061 sec/batch\n",
      "Epoch 1/2  Iteration 4/356 Training loss: 4.3482 2.0357 sec/batch\n",
      "Epoch 1/2  Iteration 5/356 Training loss: 4.2881 2.2615 sec/batch\n",
      "Epoch 1/2  Iteration 6/356 Training loss: 4.1822 2.3661 sec/batch\n",
      "Epoch 1/2  Iteration 7/356 Training loss: 4.0811 2.2308 sec/batch\n",
      "Epoch 1/2  Iteration 8/356 Training loss: 3.9894 2.4757 sec/batch\n",
      "Epoch 1/2  Iteration 9/356 Training loss: 3.9080 2.1525 sec/batch\n",
      "Epoch 1/2  Iteration 10/356 Training loss: 3.8414 2.3237 sec/batch\n",
      "Epoch 1/2  Iteration 11/356 Training loss: 3.7829 2.3937 sec/batch\n",
      "Epoch 1/2  Iteration 12/356 Training loss: 3.7336 2.1655 sec/batch\n",
      "Epoch 1/2  Iteration 13/356 Training loss: 3.6898 2.4509 sec/batch\n",
      "Epoch 1/2  Iteration 14/356 Training loss: 3.6530 2.2802 sec/batch\n",
      "Epoch 1/2  Iteration 15/356 Training loss: 3.6203 2.2717 sec/batch\n",
      "Epoch 1/2  Iteration 16/356 Training loss: 3.5907 2.7444 sec/batch\n",
      "Epoch 1/2  Iteration 17/356 Training loss: 3.5635 2.6898 sec/batch\n",
      "Epoch 1/2  Iteration 18/356 Training loss: 3.5412 2.5357 sec/batch\n",
      "Epoch 1/2  Iteration 19/356 Training loss: 3.5202 2.1729 sec/batch\n",
      "Epoch 1/2  Iteration 20/356 Training loss: 3.4993 2.5371 sec/batch\n",
      "Epoch 1/2  Iteration 21/356 Training loss: 3.4815 2.1732 sec/batch\n",
      "Epoch 1/2  Iteration 22/356 Training loss: 3.4650 2.3471 sec/batch\n",
      "Epoch 1/2  Iteration 23/356 Training loss: 3.4496 2.2621 sec/batch\n",
      "Epoch 1/2  Iteration 24/356 Training loss: 3.4357 2.1686 sec/batch\n",
      "Epoch 1/2  Iteration 25/356 Training loss: 3.4225 2.4494 sec/batch\n",
      "Epoch 1/2  Iteration 26/356 Training loss: 3.4112 2.2218 sec/batch\n",
      "Epoch 1/2  Iteration 27/356 Training loss: 3.4008 2.3042 sec/batch\n",
      "Epoch 1/2  Iteration 28/356 Training loss: 3.3901 2.3539 sec/batch\n",
      "Epoch 1/2  Iteration 29/356 Training loss: 3.3804 2.1887 sec/batch\n",
      "Epoch 1/2  Iteration 30/356 Training loss: 3.3716 2.9216 sec/batch\n",
      "Epoch 1/2  Iteration 31/356 Training loss: 3.3641 2.3188 sec/batch\n",
      "Epoch 1/2  Iteration 32/356 Training loss: 3.3560 2.4872 sec/batch\n",
      "Epoch 1/2  Iteration 33/356 Training loss: 3.3482 2.2499 sec/batch\n",
      "Epoch 1/2  Iteration 34/356 Training loss: 3.3415 2.2354 sec/batch\n",
      "Epoch 1/2  Iteration 35/356 Training loss: 3.3346 2.3861 sec/batch\n",
      "Epoch 1/2  Iteration 36/356 Training loss: 3.3286 2.2322 sec/batch\n",
      "Epoch 1/2  Iteration 37/356 Training loss: 3.3221 2.3599 sec/batch\n",
      "Epoch 1/2  Iteration 38/356 Training loss: 3.3160 2.4036 sec/batch\n",
      "Epoch 1/2  Iteration 39/356 Training loss: 3.3102 2.3772 sec/batch\n",
      "Epoch 1/2  Iteration 40/356 Training loss: 3.3049 2.3860 sec/batch\n",
      "Epoch 1/2  Iteration 41/356 Training loss: 3.2996 2.2486 sec/batch\n",
      "Epoch 1/2  Iteration 42/356 Training loss: 3.2947 2.4718 sec/batch\n",
      "Epoch 1/2  Iteration 43/356 Training loss: 3.2899 3.7784 sec/batch\n",
      "Epoch 1/2  Iteration 44/356 Training loss: 3.2853 3.6280 sec/batch\n",
      "Epoch 1/2  Iteration 45/356 Training loss: 3.2808 2.8182 sec/batch\n",
      "Epoch 1/2  Iteration 46/356 Training loss: 3.2769 2.6897 sec/batch\n",
      "Epoch 1/2  Iteration 47/356 Training loss: 3.2733 2.3866 sec/batch\n",
      "Epoch 1/2  Iteration 48/356 Training loss: 3.2699 2.3956 sec/batch\n",
      "Epoch 1/2  Iteration 49/356 Training loss: 3.2666 2.1752 sec/batch\n",
      "Epoch 1/2  Iteration 50/356 Training loss: 3.2634 2.2779 sec/batch\n",
      "Epoch 1/2  Iteration 51/356 Training loss: 3.2601 2.2689 sec/batch\n",
      "Epoch 1/2  Iteration 52/356 Training loss: 3.2569 2.1861 sec/batch\n",
      "Epoch 1/2  Iteration 53/356 Training loss: 3.2540 2.2171 sec/batch\n",
      "Epoch 1/2  Iteration 54/356 Training loss: 3.2509 2.2749 sec/batch\n",
      "Epoch 1/2  Iteration 55/356 Training loss: 3.2481 2.1982 sec/batch\n",
      "Epoch 1/2  Iteration 56/356 Training loss: 3.2451 2.2510 sec/batch\n",
      "Epoch 1/2  Iteration 57/356 Training loss: 3.2424 2.3348 sec/batch\n",
      "Epoch 1/2  Iteration 58/356 Training loss: 3.2398 2.1383 sec/batch\n",
      "Epoch 1/2  Iteration 59/356 Training loss: 3.2372 2.1324 sec/batch\n",
      "Epoch 1/2  Iteration 60/356 Training loss: 3.2349 2.3138 sec/batch\n",
      "Epoch 1/2  Iteration 61/356 Training loss: 3.2326 2.3361 sec/batch\n",
      "Epoch 1/2  Iteration 62/356 Training loss: 3.2307 2.8284 sec/batch\n",
      "Epoch 1/2  Iteration 63/356 Training loss: 3.2290 2.6390 sec/batch\n",
      "Epoch 1/2  Iteration 64/356 Training loss: 3.2265 2.2689 sec/batch\n",
      "Epoch 1/2  Iteration 65/356 Training loss: 3.2243 2.2969 sec/batch\n",
      "Epoch 1/2  Iteration 66/356 Training loss: 3.2226 2.3058 sec/batch\n",
      "Epoch 1/2  Iteration 67/356 Training loss: 3.2208 2.2700 sec/batch\n",
      "Epoch 1/2  Iteration 68/356 Training loss: 3.2183 2.2340 sec/batch\n",
      "Epoch 1/2  Iteration 69/356 Training loss: 3.2163 2.1014 sec/batch\n",
      "Epoch 1/2  Iteration 70/356 Training loss: 3.2147 2.2530 sec/batch\n",
      "Epoch 1/2  Iteration 71/356 Training loss: 3.2129 2.1022 sec/batch\n",
      "Epoch 1/2  Iteration 72/356 Training loss: 3.2114 1.9692 sec/batch\n",
      "Epoch 1/2  Iteration 73/356 Training loss: 3.2097 2.1174 sec/batch\n",
      "Epoch 1/2  Iteration 74/356 Training loss: 3.2081 2.0314 sec/batch\n",
      "Epoch 1/2  Iteration 75/356 Training loss: 3.2067 2.0535 sec/batch\n",
      "Epoch 1/2  Iteration 76/356 Training loss: 3.2054 2.0160 sec/batch\n",
      "Epoch 1/2  Iteration 77/356 Training loss: 3.2040 2.2351 sec/batch\n",
      "Epoch 1/2  Iteration 78/356 Training loss: 3.2025 2.0705 sec/batch\n",
      "Epoch 1/2  Iteration 79/356 Training loss: 3.2010 2.0894 sec/batch\n",
      "Epoch 1/2  Iteration 80/356 Training loss: 3.1994 2.0934 sec/batch\n",
      "Epoch 1/2  Iteration 81/356 Training loss: 3.1980 2.1602 sec/batch\n",
      "Epoch 1/2  Iteration 82/356 Training loss: 3.1967 2.0894 sec/batch\n",
      "Epoch 1/2  Iteration 83/356 Training loss: 3.1955 2.2181 sec/batch\n",
      "Epoch 1/2  Iteration 84/356 Training loss: 3.1941 2.0977 sec/batch\n",
      "Epoch 1/2  Iteration 85/356 Training loss: 3.1926 2.0106 sec/batch\n",
      "Epoch 1/2  Iteration 86/356 Training loss: 3.1912 2.1193 sec/batch\n",
      "Epoch 1/2  Iteration 87/356 Training loss: 3.1897 2.2580 sec/batch\n",
      "Epoch 1/2  Iteration 88/356 Training loss: 3.1883 2.0814 sec/batch\n",
      "Epoch 1/2  Iteration 89/356 Training loss: 3.1871 2.2051 sec/batch\n",
      "Epoch 1/2  Iteration 90/356 Training loss: 3.1859 2.2071 sec/batch\n",
      "Epoch 1/2  Iteration 91/356 Training loss: 3.1847 2.1732 sec/batch\n",
      "Epoch 1/2  Iteration 92/356 Training loss: 3.1834 2.2101 sec/batch\n",
      "Epoch 1/2  Iteration 93/356 Training loss: 3.1821 2.0206 sec/batch\n",
      "Epoch 1/2  Iteration 94/356 Training loss: 3.1809 2.8126 sec/batch\n",
      "Epoch 1/2  Iteration 95/356 Training loss: 3.1795 2.4463 sec/batch\n",
      "Epoch 1/2  Iteration 96/356 Training loss: 3.1782 3.3861 sec/batch\n",
      "Epoch 1/2  Iteration 97/356 Training loss: 3.1770 2.5345 sec/batch\n",
      "Epoch 1/2  Iteration 98/356 Training loss: 3.1756 2.5766 sec/batch\n",
      "Epoch 1/2  Iteration 99/356 Training loss: 3.1742 3.6095 sec/batch\n",
      "Epoch 1/2  Iteration 100/356 Training loss: 3.1728 3.9047 sec/batch\n",
      "Epoch 1/2  Iteration 101/356 Training loss: 3.1715 3.5299 sec/batch\n",
      "Epoch 1/2  Iteration 102/356 Training loss: 3.1701 3.6154 sec/batch\n",
      "Epoch 1/2  Iteration 103/356 Training loss: 3.1687 3.5980 sec/batch\n",
      "Epoch 1/2  Iteration 104/356 Training loss: 3.1672 2.4352 sec/batch\n",
      "Epoch 1/2  Iteration 105/356 Training loss: 3.1656 2.5515 sec/batch\n",
      "Epoch 1/2  Iteration 106/356 Training loss: 3.1641 2.6273 sec/batch\n",
      "Epoch 1/2  Iteration 107/356 Training loss: 3.1623 2.3300 sec/batch\n",
      "Epoch 1/2  Iteration 108/356 Training loss: 3.1607 2.3798 sec/batch\n",
      "Epoch 1/2  Iteration 109/356 Training loss: 3.1591 2.4488 sec/batch\n",
      "Epoch 1/2  Iteration 110/356 Training loss: 3.1571 2.5547 sec/batch\n",
      "Epoch 1/2  Iteration 111/356 Training loss: 3.1553 2.7725 sec/batch\n",
      "Epoch 1/2  Iteration 112/356 Training loss: 3.1536 2.3585 sec/batch\n",
      "Epoch 1/2  Iteration 113/356 Training loss: 3.1516 2.6084 sec/batch\n",
      "Epoch 1/2  Iteration 114/356 Training loss: 3.1496 2.2312 sec/batch\n",
      "Epoch 1/2  Iteration 115/356 Training loss: 3.1475 2.4479 sec/batch\n",
      "Epoch 1/2  Iteration 116/356 Training loss: 3.1454 2.4002 sec/batch\n",
      "Epoch 1/2  Iteration 117/356 Training loss: 3.1433 2.3888 sec/batch\n",
      "Epoch 1/2  Iteration 118/356 Training loss: 3.1414 2.3584 sec/batch\n",
      "Epoch 1/2  Iteration 119/356 Training loss: 3.1393 2.2736 sec/batch\n",
      "Epoch 1/2  Iteration 120/356 Training loss: 3.1370 2.4539 sec/batch\n",
      "Epoch 1/2  Iteration 121/356 Training loss: 3.1350 2.4162 sec/batch\n",
      "Epoch 1/2  Iteration 122/356 Training loss: 3.1328 2.4046 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2  Iteration 123/356 Training loss: 3.1304 2.3307 sec/batch\n",
      "Epoch 1/2  Iteration 124/356 Training loss: 3.1281 2.2349 sec/batch\n",
      "Epoch 1/2  Iteration 125/356 Training loss: 3.1256 2.4167 sec/batch\n",
      "Epoch 1/2  Iteration 126/356 Training loss: 3.1229 2.3568 sec/batch\n",
      "Epoch 1/2  Iteration 127/356 Training loss: 3.1204 2.4087 sec/batch\n",
      "Epoch 1/2  Iteration 128/356 Training loss: 3.1190 2.3709 sec/batch\n",
      "Epoch 1/2  Iteration 129/356 Training loss: 3.1180 2.2904 sec/batch\n",
      "Epoch 1/2  Iteration 130/356 Training loss: 3.1160 2.4930 sec/batch\n",
      "Epoch 1/2  Iteration 131/356 Training loss: 3.1143 2.2336 sec/batch\n",
      "Epoch 1/2  Iteration 132/356 Training loss: 3.1124 2.3918 sec/batch\n",
      "Epoch 1/2  Iteration 133/356 Training loss: 3.1103 2.3836 sec/batch\n",
      "Epoch 1/2  Iteration 134/356 Training loss: 3.1081 2.3647 sec/batch\n",
      "Epoch 1/2  Iteration 135/356 Training loss: 3.1054 2.5038 sec/batch\n",
      "Epoch 1/2  Iteration 136/356 Training loss: 3.1030 2.2718 sec/batch\n",
      "Epoch 1/2  Iteration 137/356 Training loss: 3.1005 2.5424 sec/batch\n",
      "Epoch 1/2  Iteration 138/356 Training loss: 3.0980 2.3068 sec/batch\n",
      "Epoch 1/2  Iteration 139/356 Training loss: 3.0956 2.7225 sec/batch\n",
      "Epoch 1/2  Iteration 140/356 Training loss: 3.0930 2.3922 sec/batch\n",
      "Epoch 1/2  Iteration 141/356 Training loss: 3.0905 2.4623 sec/batch\n",
      "Epoch 1/2  Iteration 142/356 Training loss: 3.0877 2.6794 sec/batch\n",
      "Epoch 1/2  Iteration 143/356 Training loss: 3.0850 2.7075 sec/batch\n",
      "Epoch 1/2  Iteration 144/356 Training loss: 3.0822 2.3912 sec/batch\n",
      "Epoch 1/2  Iteration 145/356 Training loss: 3.0795 2.2953 sec/batch\n",
      "Epoch 1/2  Iteration 146/356 Training loss: 3.0768 2.4256 sec/batch\n",
      "Epoch 1/2  Iteration 147/356 Training loss: 3.0741 2.3570 sec/batch\n",
      "Epoch 1/2  Iteration 148/356 Training loss: 3.0714 2.3955 sec/batch\n",
      "Epoch 1/2  Iteration 149/356 Training loss: 3.0684 2.4486 sec/batch\n",
      "Epoch 1/2  Iteration 150/356 Training loss: 3.0655 2.2395 sec/batch\n",
      "Epoch 1/2  Iteration 151/356 Training loss: 3.0628 2.4300 sec/batch\n",
      "Epoch 1/2  Iteration 152/356 Training loss: 3.0602 2.4109 sec/batch\n",
      "Epoch 1/2  Iteration 153/356 Training loss: 3.0573 2.4267 sec/batch\n",
      "Epoch 1/2  Iteration 154/356 Training loss: 3.0544 2.4247 sec/batch\n",
      "Epoch 1/2  Iteration 155/356 Training loss: 3.0514 2.2803 sec/batch\n",
      "Epoch 1/2  Iteration 156/356 Training loss: 3.0484 2.4022 sec/batch\n",
      "Epoch 1/2  Iteration 157/356 Training loss: 3.0453 2.2752 sec/batch\n",
      "Epoch 1/2  Iteration 158/356 Training loss: 3.0423 2.3715 sec/batch\n",
      "Epoch 1/2  Iteration 159/356 Training loss: 3.0398 2.3260 sec/batch\n",
      "Epoch 1/2  Iteration 160/356 Training loss: 3.0371 2.2288 sec/batch\n",
      "Epoch 1/2  Iteration 161/356 Training loss: 3.0344 2.4957 sec/batch\n",
      "Epoch 1/2  Iteration 162/356 Training loss: 3.0314 2.2206 sec/batch\n",
      "Epoch 1/2  Iteration 163/356 Training loss: 3.0285 2.3550 sec/batch\n",
      "Epoch 1/2  Iteration 164/356 Training loss: 3.0256 2.3579 sec/batch\n",
      "Epoch 1/2  Iteration 165/356 Training loss: 3.0228 2.3515 sec/batch\n",
      "Epoch 1/2  Iteration 166/356 Training loss: 3.0199 2.4878 sec/batch\n",
      "Epoch 1/2  Iteration 167/356 Training loss: 3.0171 2.2180 sec/batch\n",
      "Epoch 1/2  Iteration 168/356 Training loss: 3.0142 2.4347 sec/batch\n",
      "Epoch 1/2  Iteration 169/356 Training loss: 3.0115 2.3111 sec/batch\n",
      "Epoch 1/2  Iteration 170/356 Training loss: 3.0085 2.2793 sec/batch\n",
      "Epoch 1/2  Iteration 171/356 Training loss: 3.0057 2.6080 sec/batch\n",
      "Epoch 1/2  Iteration 172/356 Training loss: 3.0030 2.4168 sec/batch\n",
      "Epoch 1/2  Iteration 173/356 Training loss: 3.0003 2.4725 sec/batch\n",
      "Epoch 1/2  Iteration 174/356 Training loss: 2.9977 2.3094 sec/batch\n",
      "Epoch 1/2  Iteration 175/356 Training loss: 2.9951 2.4140 sec/batch\n",
      "Epoch 1/2  Iteration 176/356 Training loss: 2.9922 2.3680 sec/batch\n",
      "Epoch 1/2  Iteration 177/356 Training loss: 2.9894 2.2537 sec/batch\n",
      "Epoch 1/2  Iteration 178/356 Training loss: 2.9864 2.4443 sec/batch\n",
      "############################# Epoch 2 #############################\n",
      "Epoch 2/2  Iteration 179/356 Training loss: 2.5682 2.3363 sec/batch\n",
      "Epoch 2/2  Iteration 180/356 Training loss: 2.5026 2.4023 sec/batch\n",
      "Epoch 2/2  Iteration 181/356 Training loss: 2.4847 2.2863 sec/batch\n",
      "Epoch 2/2  Iteration 182/356 Training loss: 2.4758 2.3280 sec/batch\n",
      "Epoch 2/2  Iteration 183/356 Training loss: 2.4705 2.4618 sec/batch\n",
      "Epoch 2/2  Iteration 184/356 Training loss: 2.4648 2.2616 sec/batch\n",
      "Epoch 2/2  Iteration 185/356 Training loss: 2.4617 2.4580 sec/batch\n",
      "Epoch 2/2  Iteration 186/356 Training loss: 2.4597 2.3294 sec/batch\n",
      "Epoch 2/2  Iteration 187/356 Training loss: 2.4586 2.4047 sec/batch\n",
      "Epoch 2/2  Iteration 188/356 Training loss: 2.4553 2.4046 sec/batch\n",
      "Epoch 2/2  Iteration 189/356 Training loss: 2.4517 2.2460 sec/batch\n",
      "Epoch 2/2  Iteration 190/356 Training loss: 2.4498 2.4142 sec/batch\n",
      "Epoch 2/2  Iteration 191/356 Training loss: 2.4473 2.3244 sec/batch\n",
      "Epoch 2/2  Iteration 192/356 Training loss: 2.4476 2.4217 sec/batch\n",
      "Epoch 2/2  Iteration 193/356 Training loss: 2.4469 2.4226 sec/batch\n",
      "Epoch 2/2  Iteration 194/356 Training loss: 2.4468 2.2307 sec/batch\n",
      "Epoch 2/2  Iteration 195/356 Training loss: 2.4461 2.4203 sec/batch\n",
      "Epoch 2/2  Iteration 196/356 Training loss: 2.4462 2.3250 sec/batch\n",
      "Epoch 2/2  Iteration 197/356 Training loss: 2.4455 2.4659 sec/batch\n",
      "Epoch 2/2  Iteration 198/356 Training loss: 2.4427 2.3411 sec/batch\n",
      "Epoch 2/2  Iteration 199/356 Training loss: 2.4409 2.3121 sec/batch\n",
      "Epoch 2/2  Iteration 200/356 Training loss: 2.4405 2.4727 sec/batch\n",
      "Validation loss: 2.3992686 Saving checkpoint!\n",
      "Epoch 2/2  Iteration 201/356 Training loss: 2.4394 2.4504 sec/batch\n",
      "Epoch 2/2  Iteration 202/356 Training loss: 2.4374 2.4345 sec/batch\n",
      "Epoch 2/2  Iteration 203/356 Training loss: 2.4354 2.3571 sec/batch\n",
      "Epoch 2/2  Iteration 204/356 Training loss: 2.4338 2.4401 sec/batch\n",
      "Epoch 2/2  Iteration 205/356 Training loss: 2.4319 2.2836 sec/batch\n",
      "Epoch 2/2  Iteration 206/356 Training loss: 2.4301 2.4507 sec/batch\n",
      "Epoch 2/2  Iteration 207/356 Training loss: 2.4287 2.4015 sec/batch\n",
      "Epoch 2/2  Iteration 208/356 Training loss: 2.4273 2.4163 sec/batch\n",
      "Epoch 2/2  Iteration 209/356 Training loss: 2.4264 2.5216 sec/batch\n",
      "Epoch 2/2  Iteration 210/356 Training loss: 2.4243 2.3780 sec/batch\n",
      "Epoch 2/2  Iteration 211/356 Training loss: 2.4222 2.5019 sec/batch\n",
      "Epoch 2/2  Iteration 212/356 Training loss: 2.4207 2.3210 sec/batch\n",
      "Epoch 2/2  Iteration 213/356 Training loss: 2.4188 2.4632 sec/batch\n",
      "Epoch 2/2  Iteration 214/356 Training loss: 2.4176 2.4555 sec/batch\n",
      "Epoch 2/2  Iteration 215/356 Training loss: 2.4156 2.2996 sec/batch\n",
      "Epoch 2/2  Iteration 216/356 Training loss: 2.4133 2.4862 sec/batch\n",
      "Epoch 2/2  Iteration 217/356 Training loss: 2.4115 2.2378 sec/batch\n",
      "Epoch 2/2  Iteration 218/356 Training loss: 2.4098 2.5949 sec/batch\n",
      "Epoch 2/2  Iteration 219/356 Training loss: 2.4080 2.3271 sec/batch\n",
      "Epoch 2/2  Iteration 220/356 Training loss: 2.4059 2.4414 sec/batch\n",
      "Epoch 2/2  Iteration 221/356 Training loss: 2.4040 2.4082 sec/batch\n",
      "Epoch 2/2  Iteration 222/356 Training loss: 2.4019 2.3763 sec/batch\n",
      "Epoch 2/2  Iteration 223/356 Training loss: 2.4002 2.4549 sec/batch\n",
      "Epoch 2/2  Iteration 224/356 Training loss: 2.3977 2.3290 sec/batch\n",
      "Epoch 2/2  Iteration 225/356 Training loss: 2.3965 2.6360 sec/batch\n",
      "Epoch 2/2  Iteration 226/356 Training loss: 2.3948 2.3125 sec/batch\n",
      "Epoch 2/2  Iteration 227/356 Training loss: 2.3932 2.4437 sec/batch\n",
      "Epoch 2/2  Iteration 228/356 Training loss: 2.3921 2.4296 sec/batch\n",
      "Epoch 2/2  Iteration 229/356 Training loss: 2.3903 2.3463 sec/batch\n",
      "Epoch 2/2  Iteration 230/356 Training loss: 2.3891 2.4395 sec/batch\n",
      "Epoch 2/2  Iteration 231/356 Training loss: 2.3874 2.3577 sec/batch\n",
      "Epoch 2/2  Iteration 232/356 Training loss: 2.3858 2.4480 sec/batch\n",
      "Epoch 2/2  Iteration 233/356 Training loss: 2.3842 2.4228 sec/batch\n",
      "Epoch 2/2  Iteration 234/356 Training loss: 2.3829 2.4549 sec/batch\n",
      "Epoch 2/2  Iteration 235/356 Training loss: 2.3814 2.5463 sec/batch\n",
      "Epoch 2/2  Iteration 236/356 Training loss: 2.3797 2.2588 sec/batch\n",
      "Epoch 2/2  Iteration 237/356 Training loss: 2.3782 2.5049 sec/batch\n",
      "Epoch 2/2  Iteration 238/356 Training loss: 2.3771 2.3559 sec/batch\n",
      "Epoch 2/2  Iteration 239/356 Training loss: 2.3756 2.8385 sec/batch\n",
      "Epoch 2/2  Iteration 240/356 Training loss: 2.3744 2.4942 sec/batch\n",
      "Epoch 2/2  Iteration 241/356 Training loss: 2.3735 2.4853 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2  Iteration 242/356 Training loss: 2.3721 2.4906 sec/batch\n",
      "Epoch 2/2  Iteration 243/356 Training loss: 2.3707 2.2887 sec/batch\n",
      "Epoch 2/2  Iteration 244/356 Training loss: 2.3697 2.4664 sec/batch\n",
      "Epoch 2/2  Iteration 245/356 Training loss: 2.3684 2.3931 sec/batch\n",
      "Epoch 2/2  Iteration 246/356 Training loss: 2.3666 2.4584 sec/batch\n",
      "Epoch 2/2  Iteration 247/356 Training loss: 2.3650 2.4322 sec/batch\n",
      "Epoch 2/2  Iteration 248/356 Training loss: 2.3637 2.3503 sec/batch\n",
      "Epoch 2/2  Iteration 249/356 Training loss: 2.3626 2.4665 sec/batch\n",
      "Epoch 2/2  Iteration 250/356 Training loss: 2.3614 2.3181 sec/batch\n",
      "Epoch 2/2  Iteration 251/356 Training loss: 2.3601 2.4767 sec/batch\n",
      "Epoch 2/2  Iteration 252/356 Training loss: 2.3586 2.3571 sec/batch\n",
      "Epoch 2/2  Iteration 253/356 Training loss: 2.3572 2.3802 sec/batch\n",
      "Epoch 2/2  Iteration 254/356 Training loss: 2.3564 2.4256 sec/batch\n",
      "Epoch 2/2  Iteration 255/356 Training loss: 2.3549 2.3070 sec/batch\n",
      "Epoch 2/2  Iteration 256/356 Training loss: 2.3539 2.4775 sec/batch\n",
      "Epoch 2/2  Iteration 257/356 Training loss: 2.3523 2.3449 sec/batch\n",
      "Epoch 2/2  Iteration 258/356 Training loss: 2.3509 2.5320 sec/batch\n",
      "Epoch 2/2  Iteration 259/356 Training loss: 2.3494 2.3930 sec/batch\n",
      "Epoch 2/2  Iteration 260/356 Training loss: 2.3483 2.4349 sec/batch\n",
      "Epoch 2/2  Iteration 261/356 Training loss: 2.3467 2.3904 sec/batch\n",
      "Epoch 2/2  Iteration 262/356 Training loss: 2.3454 2.4124 sec/batch\n",
      "Epoch 2/2  Iteration 263/356 Training loss: 2.3438 2.4218 sec/batch\n",
      "Epoch 2/2  Iteration 264/356 Training loss: 2.3424 2.3696 sec/batch\n",
      "Epoch 2/2  Iteration 265/356 Training loss: 2.3410 2.4400 sec/batch\n",
      "Epoch 2/2  Iteration 266/356 Training loss: 2.3397 2.3489 sec/batch\n",
      "Epoch 2/2  Iteration 267/356 Training loss: 2.3383 2.3097 sec/batch\n",
      "Epoch 2/2  Iteration 268/356 Training loss: 2.3371 2.4577 sec/batch\n",
      "Epoch 2/2  Iteration 269/356 Training loss: 2.3358 2.2918 sec/batch\n",
      "Epoch 2/2  Iteration 270/356 Training loss: 2.3346 2.5291 sec/batch\n",
      "Epoch 2/2  Iteration 271/356 Training loss: 2.3331 2.3181 sec/batch\n",
      "Epoch 2/2  Iteration 272/356 Training loss: 2.3317 2.5362 sec/batch\n",
      "Epoch 2/2  Iteration 273/356 Training loss: 2.3302 2.4076 sec/batch\n",
      "Epoch 2/2  Iteration 274/356 Training loss: 2.3288 2.2891 sec/batch\n",
      "Epoch 2/2  Iteration 275/356 Training loss: 2.3274 2.4876 sec/batch\n",
      "Epoch 2/2  Iteration 276/356 Training loss: 2.3260 2.2633 sec/batch\n",
      "Epoch 2/2  Iteration 277/356 Training loss: 2.3246 2.4980 sec/batch\n",
      "Epoch 2/2  Iteration 278/356 Training loss: 2.3232 2.5229 sec/batch\n",
      "Epoch 2/2  Iteration 279/356 Training loss: 2.3220 2.8139 sec/batch\n",
      "Epoch 2/2  Iteration 280/356 Training loss: 2.3209 2.4428 sec/batch\n",
      "Epoch 2/2  Iteration 281/356 Training loss: 2.3195 2.3483 sec/batch\n",
      "Epoch 2/2  Iteration 282/356 Training loss: 2.3183 2.4774 sec/batch\n",
      "Epoch 2/2  Iteration 283/356 Training loss: 2.3169 2.4985 sec/batch\n",
      "Epoch 2/2  Iteration 284/356 Training loss: 2.3157 2.5652 sec/batch\n",
      "Epoch 2/2  Iteration 285/356 Training loss: 2.3144 2.3768 sec/batch\n",
      "Epoch 2/2  Iteration 286/356 Training loss: 2.3134 2.4226 sec/batch\n",
      "Epoch 2/2  Iteration 287/356 Training loss: 2.3123 2.3731 sec/batch\n",
      "Epoch 2/2  Iteration 288/356 Training loss: 2.3110 2.3389 sec/batch\n",
      "Epoch 2/2  Iteration 289/356 Training loss: 2.3099 2.5867 sec/batch\n",
      "Epoch 2/2  Iteration 290/356 Training loss: 2.3088 2.3069 sec/batch\n",
      "Epoch 2/2  Iteration 291/356 Training loss: 2.3075 2.4929 sec/batch\n",
      "Epoch 2/2  Iteration 292/356 Training loss: 2.3062 2.3201 sec/batch\n",
      "Epoch 2/2  Iteration 293/356 Training loss: 2.3050 2.3988 sec/batch\n",
      "Epoch 2/2  Iteration 294/356 Training loss: 2.3035 2.5087 sec/batch\n",
      "Epoch 2/2  Iteration 295/356 Training loss: 2.3023 2.3335 sec/batch\n",
      "Epoch 2/2  Iteration 296/356 Training loss: 2.3011 2.4550 sec/batch\n",
      "Epoch 2/2  Iteration 297/356 Training loss: 2.3000 2.3462 sec/batch\n",
      "Epoch 2/2  Iteration 298/356 Training loss: 2.2989 2.4895 sec/batch\n",
      "Epoch 2/2  Iteration 299/356 Training loss: 2.2978 2.3773 sec/batch\n",
      "Epoch 2/2  Iteration 300/356 Training loss: 2.2966 2.3996 sec/batch\n",
      "Epoch 2/2  Iteration 301/356 Training loss: 2.2953 2.4664 sec/batch\n",
      "Epoch 2/2  Iteration 302/356 Training loss: 2.2944 2.3187 sec/batch\n",
      "Epoch 2/2  Iteration 303/356 Training loss: 2.2932 2.4498 sec/batch\n",
      "Epoch 2/2  Iteration 304/356 Training loss: 2.2919 2.5041 sec/batch\n",
      "Epoch 2/2  Iteration 305/356 Training loss: 2.2909 2.3823 sec/batch\n",
      "Epoch 2/2  Iteration 306/356 Training loss: 2.2899 2.3959 sec/batch\n",
      "Epoch 2/2  Iteration 307/356 Training loss: 2.2888 2.3946 sec/batch\n",
      "Epoch 2/2  Iteration 308/356 Training loss: 2.2876 2.5731 sec/batch\n",
      "Epoch 2/2  Iteration 309/356 Training loss: 2.2864 2.3942 sec/batch\n",
      "Epoch 2/2  Iteration 310/356 Training loss: 2.2851 2.4258 sec/batch\n",
      "Epoch 2/2  Iteration 311/356 Training loss: 2.2840 2.4350 sec/batch\n",
      "Epoch 2/2  Iteration 312/356 Training loss: 2.2829 2.4840 sec/batch\n",
      "Epoch 2/2  Iteration 313/356 Training loss: 2.2817 2.4978 sec/batch\n",
      "Epoch 2/2  Iteration 314/356 Training loss: 2.2807 2.4457 sec/batch\n",
      "Epoch 2/2  Iteration 315/356 Training loss: 2.2796 2.5047 sec/batch\n",
      "Epoch 2/2  Iteration 316/356 Training loss: 2.2785 2.3716 sec/batch\n",
      "Epoch 2/2  Iteration 317/356 Training loss: 2.2776 2.4442 sec/batch\n",
      "Epoch 2/2  Iteration 318/356 Training loss: 2.2764 2.3609 sec/batch\n",
      "Epoch 2/2  Iteration 319/356 Training loss: 2.2755 2.4229 sec/batch\n",
      "Epoch 2/2  Iteration 320/356 Training loss: 2.2743 2.3647 sec/batch\n",
      "Epoch 2/2  Iteration 321/356 Training loss: 2.2732 2.4224 sec/batch\n",
      "Epoch 2/2  Iteration 322/356 Training loss: 2.2721 2.4463 sec/batch\n",
      "Epoch 2/2  Iteration 323/356 Training loss: 2.2709 2.3517 sec/batch\n",
      "Epoch 2/2  Iteration 324/356 Training loss: 2.2700 2.4250 sec/batch\n",
      "Epoch 2/2  Iteration 325/356 Training loss: 2.2690 2.3786 sec/batch\n",
      "Epoch 2/2  Iteration 326/356 Training loss: 2.2680 2.4092 sec/batch\n",
      "Epoch 2/2  Iteration 327/356 Training loss: 2.2669 2.4315 sec/batch\n",
      "Epoch 2/2  Iteration 328/356 Training loss: 2.2657 2.3765 sec/batch\n",
      "Epoch 2/2  Iteration 329/356 Training loss: 2.2647 2.4101 sec/batch\n",
      "Epoch 2/2  Iteration 330/356 Training loss: 2.2639 2.3125 sec/batch\n",
      "Epoch 2/2  Iteration 331/356 Training loss: 2.2629 2.4872 sec/batch\n",
      "Epoch 2/2  Iteration 332/356 Training loss: 2.2620 2.3640 sec/batch\n",
      "Epoch 2/2  Iteration 333/356 Training loss: 2.2610 2.4778 sec/batch\n",
      "Epoch 2/2  Iteration 334/356 Training loss: 2.2600 2.4571 sec/batch\n",
      "Epoch 2/2  Iteration 335/356 Training loss: 2.2589 2.4388 sec/batch\n",
      "Epoch 2/2  Iteration 336/356 Training loss: 2.2578 2.4164 sec/batch\n",
      "Epoch 2/2  Iteration 337/356 Training loss: 2.2565 2.3204 sec/batch\n",
      "Epoch 2/2  Iteration 338/356 Training loss: 2.2557 2.4577 sec/batch\n",
      "Epoch 2/2  Iteration 339/356 Training loss: 2.2547 2.4836 sec/batch\n",
      "Epoch 2/2  Iteration 340/356 Training loss: 2.2536 2.3758 sec/batch\n",
      "Epoch 2/2  Iteration 341/356 Training loss: 2.2526 2.4724 sec/batch\n",
      "Epoch 2/2  Iteration 342/356 Training loss: 2.2515 2.6590 sec/batch\n",
      "Epoch 2/2  Iteration 343/356 Training loss: 2.2505 2.8542 sec/batch\n",
      "Epoch 2/2  Iteration 344/356 Training loss: 2.2494 2.8098 sec/batch\n",
      "Epoch 2/2  Iteration 345/356 Training loss: 2.2484 2.6735 sec/batch\n",
      "Epoch 2/2  Iteration 346/356 Training loss: 2.2475 2.4282 sec/batch\n",
      "Epoch 2/2  Iteration 347/356 Training loss: 2.2464 2.5422 sec/batch\n",
      "Epoch 2/2  Iteration 348/356 Training loss: 2.2453 2.4167 sec/batch\n",
      "Epoch 2/2  Iteration 349/356 Training loss: 2.2442 2.3907 sec/batch\n",
      "Epoch 2/2  Iteration 350/356 Training loss: 2.2432 2.4714 sec/batch\n",
      "Epoch 2/2  Iteration 351/356 Training loss: 2.2424 2.4350 sec/batch\n",
      "Epoch 2/2  Iteration 352/356 Training loss: 2.2424 2.4185 sec/batch\n",
      "Epoch 2/2  Iteration 353/356 Training loss: 2.2424 2.3349 sec/batch\n",
      "Epoch 2/2  Iteration 354/356 Training loss: 2.2420 2.4298 sec/batch\n",
      "Epoch 2/2  Iteration 355/356 Training loss: 2.2412 2.4161 sec/batch\n",
      "Epoch 2/2  Iteration 356/356 Training loss: 2.2407 2.4027 sec/batch\n",
      "Validation loss: 2.1198893 Saving checkpoint!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        print('############################# Epoch {} #############################'.format(e+1))\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}_v{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y2IqZfaQEpYe",
    "outputId": "fed48c6c-9e13-45ad-ad25-6533c231651e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints\\\\i356_l512_v2.120.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i200_l512_v2.399.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i356_l512_v2.120.ckpt\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print all saved checkpoints\n",
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints\\\\i356_l512_v2.120.ckpt'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UkNSCA1kEpYo"
   },
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qHmI7f-lEpYp"
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D7Vi-53vEpYv"
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yEd9sJWzEpZC"
   },
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RrzZCDFqEpZC",
    "outputId": "66ef67a3-61f0-462a-8278-a22964edfbb8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\i356_l512_v2.120.ckpt\n",
      "Ok heel at her thee withedere wast thas\n",
      "sur toled wat it tho she paned to tho ghe staid th ille te tout to the\n",
      "treast one him and wo te perined shis the wand sald wat he taill to the tat ours oferton tha dortely on tore to the wares, and in tha tintir, she tal wind too the stitel withe trie h ofrad the wat had why than ig hit le till to tele, and who dad to head war southell wall to the tall to to the wind an tee war that whith the past is tely the wall tor the comed ane tor whal wot his was ald, and the wathel tit or worly ter tout the soling an tout ther sionted tha lithing tha ding to har thy toll the red on he persenen to to lear thas shomring on the pand that ald il hor thing to sat to s out and atrensing, and tore the sad the site tho gering astererind,\"\n",
      "\"Yould and ate to hes in whot will wo sering the tast to win t ant a tele at to lo the simen toorsintereraty thairs ittored to cherstere hat herres the sad somen attoren attherer wint an wally the sald tit to thow st te pring and tal ofer of intingaterer antereratith that inte to wes it that she\n",
      "would anderedinn to hat ant itt ore here at ard oned in stoully\n",
      "tottire to to the thit ling toren tha sant it to tho gately and the sonde warling tortant of the tald tho torle thar\n",
      "ise wart herensint ate herre saly ande there tore the saing the\n",
      "sile to be ind west toren and the telint of the tratt to har sored on that here wishe serisstere tho that ho thare shard shat aspat illadis whard the with south on the paid tourer in shis ta sis and she that what we le s and tat of oflit one hearle, thar thit the peres oud tooll wis toly the sont of the s ated ator, atrot ho seall ather teely.\"\n",
      "\n",
      "\"Yet, and har wong thin wor ato atrengising, and werlerstongeras in soot to wer tarer, in whis dore with tored toret and\n",
      "whar were wase tor tare tore,\n",
      "atr the sithe res ofle wot and to the weat to toong ato hite to wat tho sat to has toull thet hard was tould tit ale was\n",
      "ond on to hers at oft hit the somen the till ware to te attine saiss in\n"
     ]
    }
   ],
   "source": [
    "# Change the name of latest checkpoint accordingly\n",
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Ok \") # starting word = 'Ok '\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Recurrent Neural Network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
